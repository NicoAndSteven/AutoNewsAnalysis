import requests
import os
import re
from newspaper import Article

os.environ['HTTP_PROXY'] = 'http://127.0.0.1:10809'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:10809'

def sanitize_filename(filename):
    """ æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸å…è®¸çš„å­—ç¬¦ """
    return re.sub(r'[\\/*?:"<>|]', "", filename)

def fetch_article_content(url):
    """
    è·å–æ–°é—»ç½‘é¡µçš„æ­£æ–‡å†…å®¹ï¼Œå¹¶å°†å†…å®¹ä¿å­˜åˆ°é¡¹ç›®ä¸‹çš„ news æ–‡ä»¶å¤¹ä¸­
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                      'AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/58.0.3029.110 Safari/537.3'
    }
    try:
        # æ‰‹åŠ¨ä¸‹è½½é¡µé¢å†…å®¹ï¼ˆä¾¿äºæ·»åŠ  UA å’Œä»£ç†ï¼‰
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        # è§£æç½‘é¡µå†…å®¹
        article = Article(url)
        article.download(input_html=response.text)
        article.parse()
        content = article.text.strip()

        # ç¡®ä¿ news æ–‡ä»¶å¤¹å­˜åœ¨
        news_folder = "news"
        os.makedirs(news_folder, exist_ok=True)

        # ä½¿ç”¨æ–‡ç« æ ‡é¢˜ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼›æ²¡æœ‰æ ‡é¢˜åˆ™ä½¿ç”¨é»˜è®¤åç§°
        title = article.title if article.title else "news_article"
        safe_title = sanitize_filename(title)
        file_path = os.path.join(news_folder, f"{safe_title}.txt")

        # ä¿å­˜æ­£æ–‡å†…å®¹
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"âœ… æ­£æ–‡å·²ä¿å­˜åˆ° {file_path}")

        return content
    except Exception as e:
        print(f"âš ï¸ å†…å®¹æå–å¤±è´¥: {e}")
        return None

def fetch_top_headlines(country="us"):
    url = "https://newsapi.org/v2/top-headlines"
    params = {
        "country": country,
        "apiKey": "1b9bc1a956cf439a956d145672724762",
        "q": "Trump"
    }

    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        if data.get("status") == "ok":
            for i, article in enumerate(data.get("articles", []), start=1):
                content = fetch_article_content(article.get('url'))
                title = article.get('title', f"news_{i}")
                if content:
                    print(f"ğŸ“° æ–°é—» {i}: {title}")
                    print(f"ğŸ“ æè¿°: {article.get('description')}")
                    print(f"ğŸ”— é“¾æ¥: {article.get('url')}\n")
                    # print(f"\nğŸ“– å†…å®¹æ‘˜è¦ï¼ˆå‰100å­—ç¬¦ï¼‰:\n{content[:100]}...")
                else:
                    print("âŒ æ— æ³•è·å–æ­£æ–‡å†…å®¹\n")
                    continue


        else:
            print("æ¥å£é”™è¯¯:", data.get("message"))

    except requests.exceptions.RequestException as e:
        print("ç½‘ç»œè¯·æ±‚å¤±è´¥:", e)

if __name__ == "__main__":
    fetch_top_headlines("us")
